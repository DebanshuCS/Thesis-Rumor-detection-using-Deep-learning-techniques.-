{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9766 unique tokens.\n",
      "Shape of data tensor: (16865, 1000)\n",
      "Shape of label tensor: (16865, 7)\n",
      "Size of train, test: 13492 3373\n",
      "['mixture', 'true', 'mfalse', 'false', 'mtrue', 'undetermined', 'legend'] in train, test :\n",
      "[2101. 1144. 2667. 7027.  393.  126.   34.]\n",
      "[ 517.  336.  652. 1738.   94.   25.   11.]\n",
      "Total 400000 word vectors in Glove.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 50)          488350    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1000, 32)          8032      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 500, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 638,309\n",
      "Trainable params: 638,109\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "13492/13492 [==============================] - 237s 18ms/step - loss: 0.2983 - acc: 0.8733\n",
      "Epoch 2/5\n",
      "13492/13492 [==============================] - 218s 16ms/step - loss: 0.2573 - acc: 0.8929\n",
      "Epoch 3/5\n",
      "13492/13492 [==============================] - 223s 17ms/step - loss: 0.2084 - acc: 0.9176\n",
      "Epoch 4/5\n",
      "13492/13492 [==============================] - 227s 17ms/step - loss: 0.1594 - acc: 0.9396\n",
      "Epoch 5/5\n",
      "13492/13492 [==============================] - 227s 17ms/step - loss: 0.1226 - acc: 0.9548\n",
      "Correct predictions: 2983\n",
      "Total number of test examples: 3373\n",
      "Accuracy of model:  0.8843759264749481\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import LSTM, GRU\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "data_train = pd.read_csv('snopes.csv')\n",
    "\n",
    "# Input Data preprocessing\n",
    "list_labels = list(set(data_train.claim_label))\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for i in range(data_train.topic.shape[0]):\n",
    "    text1 = str(data_train.topic[i])\n",
    "    text = str(data_train.claim[i])\n",
    "    texts.append(text+text1)\n",
    "    labels.append(list_labels.index(data_train.claim_label[i]))\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Pad input sequences\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels),num_classes = len(list_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Train test validation Split\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train, x_test, y_train, y_test = train_test_split( data, labels, test_size=0.20, random_state=42)\n",
    "print('Size of train, test:', len(y_train), len(y_test))\n",
    "\n",
    "print(list_labels, 'in train, test :')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_test.sum(axis=0))\n",
    "\n",
    "#Using Pre-trained word embeddings\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Total %s word vectors in Glove.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "modell = Sequential()\n",
    "modell.add(embedding_layer)\n",
    "modell.add(Dropout(0.2))\n",
    "modell.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "modell.add(MaxPooling1D(pool_size=2))\n",
    "modell.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "modell.add(BatchNormalization())\n",
    "modell.add(Dense(256, activation='relu'))\n",
    "modell.add(Dense(128, activation='relu'))\n",
    "modell.add(Dense(64, activation='relu'))\n",
    "modell.add(Dense(32, activation='relu'))\n",
    "modell.add(Dense(len(list_labels), activation='softmax'))\n",
    "modell.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(modell.summary())\n",
    "modell.fit(x_train, y_train, epochs=5, batch_size=128)\n",
    "\n",
    "# Test model\n",
    "test_preds = modell.predict(x_test)\n",
    "preds = []\n",
    "actual = []\n",
    "correct_predictions = 0\n",
    "for i in range(len(test_preds)):\n",
    "    x = np.argmax(test_preds[i])\n",
    "    y = np.argmax(y_test[i])\n",
    "    preds.append(x)\n",
    "    actual.append(y)\n",
    "    if x==y:\n",
    "        correct_predictions+=1\n",
    "print(\"Correct predictions:\", correct_predictions)\n",
    "print(\"Total number of test examples:\", len(y_test))\n",
    "print(\"Accuracy of model: \", correct_predictions/float(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493 432 706 1665 77 0 0\n"
     ]
    }
   ],
   "source": [
    "print(preds.count(0), preds.count(1), preds.count(2), preds.count(3), preds.count(4), preds.count(5), preds.count(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 336 652 1738 94 25 11\n"
     ]
    }
   ],
   "source": [
    "print(actual.count(0), actual.count(1), actual.count(2), actual.count(3), actual.count(4), actual.count(5), actual.count(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
